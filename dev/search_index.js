var documenterSearchIndex = {"docs":
[{"location":"cli/#Cli-Interface","page":"CLI Interface","title":"Cli Interface","text":"","category":"section"},{"location":"cli/","page":"CLI Interface","title":"CLI Interface","text":"The cli Interface can be accessed globaly after running julia --project build/build.jl install.","category":"page"},{"location":"cli/","page":"CLI Interface","title":"CLI Interface","text":"\t~/.julia/bin/mlnanoshaper train [options] [flags]","category":"page"},{"location":"cli/","page":"CLI Interface","title":"CLI Interface","text":"Train a model.","category":"page"},{"location":"cli/#Intro","page":"CLI Interface","title":"Intro","text":"","category":"section"},{"location":"cli/","page":"CLI Interface","title":"CLI Interface","text":"Train a model that can reconstruct a protein surface using Machine Learning. Default value of parameters are specified in the param/param.toml file. In order to override the param, you can use the differents options. ","category":"page"},{"location":"cli/#Options","page":"CLI Interface","title":"Options","text":"","category":"section"},{"location":"cli/","page":"CLI Interface","title":"CLI Interface","text":"--nb-epoch <Int>: the number of epoch to compute.\n--model, -m <String>: the model name. Can be anakin.\n--nb-data-points <Int>: the number of proteins in the dataset to use\n--name, -n <String>: name of the training run\n--cutoff-radius, -c <Float32>: the cutoff_radius used in training\n--ref-distance <Float32>: the reference distane (in A) used to rescale distance to surface in loss\n--learning-rate, -l <Float64>: the learning rate use by the model in training.\n--loss <String>: the loss function, one of \"categorical\" or \"continuous\".","category":"page"},{"location":"cli/#Flags","page":"CLI Interface","title":"Flags","text":"","category":"section"},{"location":"cli/","page":"CLI Interface","title":"CLI Interface","text":"--gpu, -g: should we do the training on the gpu, does nothing currently.","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"CurrentModule = MLNanoShaper","category":"page"},{"location":"loss/#Building-custom-loss-functions","page":"Custom Loss","title":"Building custom loss functions","text":"","category":"section"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"A loss function is a function that take as input a lux layer, the parameters and state, a named tuple containing the points where the model is evaluated, the preprocessed input and the algebric distance to the surface. The loss function then return 2 values:","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"the loss : a scalar number\nthe state of the model\na nambed tuple containing evaluations metrics","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"function custom_loss(model,\n        ps,\n        st,\n        (;  inputs,\n            d_reals))::Tuple{\n        Float32, Any, CategoricalMetric}\n    # model evaluation\n    v_pred, st = Lux.apply(model, inputs, ps, st)\n    v_pred = vcat(v_pred, 1 .- v_pred)\n    v_pred = cpu_device()(v_pred)\n    probabilities = ignore_derivatives() do\n        generate_true_probabilities(d_reals)\n    end\n    (KL(probabilities, v_pred) |> mean,\n        st, (;))\nend","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"Once we have the loss function we need to register it in order to use in at the command line level.","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"First we need a type to represent the loss function.","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"struct CustomLoss <: LossType end","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"Then we need to give the type of metric used by the model. In our case it is a empty NamedTuple.","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"_metric_type(::Type{CustomLoss}) = @NamedTuple{}","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"We need to associate the loss function to our new type.","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"get_loss_fn(::CustomLoss) = custom_loss","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"At the end we need to give the name that will be used at the command line level to select our loss.","category":"page"},{"location":"loss/","page":"Custom Loss","title":"Custom Loss","text":"_get_loss_type(::StaticSymbol{:custom}) = CustomLoss()","category":"page"},{"location":"model/#Building-a-new-model","page":"Building Custom Models","title":"Building a new model","text":"","category":"section"},{"location":"model/","page":"Building Custom Models","title":"Building Custom Models","text":"In order to create a new model, we need to create a new function that returns a Lux.Abstractlayer in the MLNanoShaperModule. The function must take as input at least a name, van_der_waals_channel, on_gpu,and cutoff_radius.","category":"page"},{"location":"model/","page":"Building Custom Models","title":"Building Custom Models","text":"function custom_angular_dense(; name::String,\n        van_der_waals_channel = false, on_gpu = true, cutoff_radius::Float32 = 3.0f0)\n    main_chain = Parallel(.*,\n            Chain(Dense(6 => 10, elu),\n                Dense(10 => 5, elu)),\n            Lux.WrappedFunction(scale_factor)\n    )\n    main_chain = DeepSet(Chain(\n        symetrise(; cutoff_radius, device = on_gpu ? gpu_device() : identity),\n        main_chain\n    ))\n    secondary_chain = Chain(\n            BatchNorm(5),\n            Dense(5  => 10, elu),\n            Dense(10 => 1, sigmoid_fast));\n    Chain(PreprocessingLayer(Partial(select_and_preprocess; cutoff_radius)),\n        main_chain,\n        secondary_chain;\n        name)\nend","category":"page"},{"location":"model/","page":"Building Custom Models","title":"Building Custom Models","text":"Once this is done you can call the model by using the flag --model with the name of the function created. In our case --model custom_angular_dense.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MLNanoShaper","category":"page"},{"location":"#MLNanoShaper","page":"Home","title":"MLNanoShaper","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for MLNanoShaper. MLNanoShaper is a machine learning algorithm that can compute the surface of proteins.   There are multiple ways to interface with the software.","category":"page"},{"location":"","page":"Home","title":"Home","text":"As julia Modules MLNanoShaper and MLNanoShaperRunner\nAs a cli command mlnanoshaper in ~/.julia/bin.\nAs a training script script/training.bash that run multiple training runs. Requires parallel.\nRunning only: as a .so object.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [MLNanoShaper,MLNanoShaperRunner]","category":"page"},{"location":"#MLNanoShaper.AccumulatorLogger","page":"Home","title":"MLNanoShaper.AccumulatorLogger","text":"accumulator(processing,logger)\n\nA processing logger that transform logger on multiple batches Ca be used to liss numerical data, for logging to TensorBoardLogger.\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaper.AuxiliaryParameters","page":"Home","title":"MLNanoShaper.AuxiliaryParameters","text":"AuxiliaryParameters\n\nThe variables that do not influence the outome of the training run. This include the nb_epoch.\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaper.LossType","page":"Home","title":"MLNanoShaper.LossType","text":"abstract type LossType end\n\nLossType is an interface for defining loss functions.\n\nImplementation\n\ngetlossfn(::LossType)::Function : the associated loss function\nmetrictype(::Type{<:LossType)::Type : the type of metrics returned by the loss function\ngetlosstype(::StaticSymbol)::LossType : the function generating the losstype\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaper.TrainingData","page":"Home","title":"MLNanoShaper.TrainingData","text":"Training information used in model training.\n\nFields\n\natoms: the set of atoms used as model input\nskin : the Surface generated by Nanoshaper\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaper.TrainingParameters","page":"Home","title":"MLNanoShaper.TrainingParameters","text":"TrainingParameters\n\nThe training parameters used in the model training. Default values are in the param file. The training is deterministric. Theses values are hased to determine a training run\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaper._train-Tuple{MLNanoShaper.TrainingParameters, MLNanoShaper.AuxiliaryParameters}","page":"Home","title":"MLNanoShaper._train","text":"_train(training_parameters::TrainingParameters, directories::AuxiliaryParameters)\n\ntrain the model given TrainingParameters and AuxiliaryParameters.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper._train-Tuple{Tuple{MLUtils.AbstractDataContainer, MLUtils.AbstractDataContainer}, Lux.Training.TrainState, MLNanoShaper.TrainingParameters, MLNanoShaper.AuxiliaryParameters}","page":"Home","title":"MLNanoShaper._train","text":"train((train_data,test_data),training_states; nb_epoch)\n\ntrain the model on the data with nb_epoch\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.categorical_loss-NTuple{4, Any}","page":"Home","title":"MLNanoShaper.categorical_loss","text":"categorical_loss(model, ps, st, (; point, atoms, d_real))\n\nThe loss function used by in training. Return the KL divergence between true probability and empirical probability Return the error with the espected distance as a metric.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.comonicon_install-Tuple{}","page":"Home","title":"MLNanoShaper.comonicon_install","text":"comonicon_install(;kwargs...)\n\nInstall the CLI manually. This will use the default configuration in Comonicon.toml, if it exists. For more detailed reference, please refer to Comonicon documentation.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.comonicon_install_path-Tuple{}","page":"Home","title":"MLNanoShaper.comonicon_install_path","text":"comonicon_install_path(;[yes=false])\n\nInstall the PATH and FPATH to your shell configuration file. You can use comonicon_install_path(;yes=true) to skip interactive prompt. For more detailed reference, please refer to Comonicon documentation.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.continus_loss-NTuple{4, Any}","page":"Home","title":"MLNanoShaper.continus_loss","text":"continus_loss(model, ps, st, (; point, atoms, d_real))\n\nThe loss function used by in training. compare the predicted (square) distance with frac1 + \tanh(d)2 Return the error with the espected distance as a metric.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.generate_data-Tuple{}","page":"Home","title":"MLNanoShaper.generate_data","text":"generate_data()\n\ngenerate data from the parameters files in param/ by downloading the pdb files and running Nanoshaper. \n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.generate_data_points-Tuple{LuxCore.AbstractExplicitLayer, AbstractVector{GeometryBasics.Point{3, Float32}}, MLNanoShaper.TreeTrainingData{Float32}, MLNanoShaper.TrainingParameters}","page":"Home","title":"MLNanoShaper.generate_data_points","text":"generate_data_points(\n    preprocessing::Lux.AbstractExplicitLayer, points::AbstractVector{<:Point3},\n    (; atoms, skin)::TreeTrainingData{Float32}, (; ref_distance)::TrainingParameters)\n\ngenerate the data_points for a set of positions points on one protein.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.implicit_surface-Union{Tuple{T}, Tuple{MLNanoShaperRunner.AnnotedKDTree{GeometryBasics.Sphere{T}, :center, GeometryBasics.Point3{T}}, Lux.StatefulLuxLayer, Any}} where T","page":"Home","title":"MLNanoShaper.implicit_surface","text":"implicit_surface(atoms::AnnotedKDTree{Sphere{T}, :center, Point3{T}},\n    model::Lux.StatefulLuxLayer, (;\n        cutoff_radius, step)) where {T}\n\nCreate a mesh form the isosurface of function `pos -> model(atoms,pos)` using marching cubes algorithm and using step size `step`.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.load_data_pdb-Tuple{Type{<:Number}, String}","page":"Home","title":"MLNanoShaper.load_data_pdb","text":"load_data_pdb(T, name::String)\n\nLoad a TrainingData{T} from current directory. You should have a pdb and an off file with name name in current directory.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.load_data_pqr-Tuple{Type{<:Number}, String}","page":"Home","title":"MLNanoShaper.load_data_pqr","text":"load_data_pqr(T, name::String)\n\nLoad a TrainingData{T} from current directory. You should have a pdb and an off file with name name in current directory.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaper.train","page":"Home","title":"MLNanoShaper.train","text":"train [options] [flags]\n\nTrain a model.\n\nIntro\n\nTrain a model that can reconstruct a protein surface using Machine Learning. Default value of parameters are specified in the param/param.toml file. In order to override the param, you can use the differents options. \n\nOptions\n\n--nb-epoch <Int>: the number of epoch to compute.\n--model, -m <String>: the model name. Can be anakin.\n--nb-data-points <Int>: the number of proteins in the dataset to use\n--name, -n <String>: name of the training run\n--cutoff-radius, -c <Float32>: the cutoff_radius used in training\n--ref-distance <Float32>: the reference distane (in A) used to rescale distance to surface in loss\n--learning-rate, -l <Float64>: the learning rate use by the model in training.\n--loss <String>: the loss function, one of \"categorical\" or \"continuous\".\n\nFlags\n\n--gpu, -g: should we do the training on the gpu, does nothing currently.\n\n\n\n\n\n","category":"function"},{"location":"#MLNanoShaperRunner.Option","page":"Home","title":"MLNanoShaperRunner.Option","text":"state\n\nThe global state manipulated by the c interface. To use, you must first load the weights using load_weights and the input atoms using load_atoms. Then you can call eval_model to get the field on a certain point.\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaperRunner.AnnotedKDTree","page":"Home","title":"MLNanoShaperRunner.AnnotedKDTree","text":"AnnotedKDTree(data::StructVector,property::StaticSymbol)\n\nFields\n\ndata::StructVector\ntree::KDTree\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaperRunner.ConcatenatedBatch","page":"Home","title":"MLNanoShaperRunner.ConcatenatedBatch","text":"ConcatenatedBatch\n\nRepresent a vector of arrays of sizes (a..., bn) where bn is the variable dimension of the batch. You can access view of individual arrays with get_slice.\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaperRunner.ModelInput","page":"Home","title":"MLNanoShaperRunner.ModelInput","text":"ModelInput\n\ninput of the model\n\nFields\n\npoint::Point3, the position of the input\natoms::StructVector{Sphere}, the atoms in the neighboord\n\n\n\n\n\n","category":"type"},{"location":"#MLNanoShaperRunner.batched_sum-Tuple{AbstractMatrix, AbstractVector}","page":"Home","title":"MLNanoShaperRunner.batched_sum","text":"batched_sum(b::AbstractMatrix,nb_elements::AbstractVector)\n\ncompute the sum of a Concatenated batch with ndim  = 2. The first dim is the feature dimension. The second dim is the the batch dim.\n\nGiven b of size (n,m) and nb_elements of size (k,), the output has size (n,k).\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaperRunner.distance-Tuple{AbstractVector{<:AbstractVector}, NearestNeighbors.KDTree}","page":"Home","title":"MLNanoShaperRunner.distance","text":"distance(x::GeometryBasics.Mesh, y::KDTree)\n\nReturn the Hausdorff distance betwen the mesh coordinates\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaperRunner.eval_model","page":"Home","title":"MLNanoShaperRunner.eval_model","text":"eval_model(x::Float32,y::Float32,z::Float32)::Float32\n\nevaluate the model at coordinates x y z.\n\n\n\n\n\n","category":"function"},{"location":"#MLNanoShaperRunner.evaluate_model-Tuple{Lux.StatefulLuxLayer, GeometryBasics.Point{3, Float32}, MLNanoShaperRunner.AnnotedKDTree}","page":"Home","title":"MLNanoShaperRunner.evaluate_model","text":"evaluate_model(\n    model::Lux.StatefulLuxLayer, x::Point3f, atoms::AnnotedKDTree; cutoff_radius, default_value = -0.0f0)\n\nevaluate the model on a single point.\nThis function handle the logic in case the point is too far from the atoms. In this case default_value is returned and the model is not run.\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaperRunner.load_atoms","page":"Home","title":"MLNanoShaperRunner.load_atoms","text":"load_atoms(start::Ptr{CSphere},length::Cint)::Cint\n\nLoad the atoms into the julia model. Start is a pointer to the start of the array of CSphere and length is the length of the array\n\nReturn an error status:\n\n0: OK\n1: data could not be read\n2: unknow error\n\n\n\n\n\n","category":"function"},{"location":"#MLNanoShaperRunner.load_model","page":"Home","title":"MLNanoShaperRunner.load_model","text":"load_model(path::String)::Cint\n\nLoad the model from a MLNanoShaperRunner.SerializedModel serialized state at absolute path path.\n\nReturn an error status:\n\n0: OK\n1: file not found\n2: file could not be deserialized properly\n3: unknow error\n\n\n\n\n\n","category":"function"},{"location":"#MLNanoShaperRunner.signed_distance-Union{Tuple{T}, Tuple{GeometryBasics.Point3{T}, MLNanoShaperRunner.RegionMesh}} where T<:Number","page":"Home","title":"MLNanoShaperRunner.signed_distance","text":"signed_distance(p::Point3, mesh::RegionMesh)::Number\n\nreturns the signed distance between point p and the mesh\n\n\n\n\n\n","category":"method"},{"location":"#MLNanoShaperRunner.tiny_angular_dense-Tuple{}","page":"Home","title":"MLNanoShaperRunner.tiny_angular_dense","text":"tiny_angular_dense(; categorical=false, van_der_waals_channel=false, kargs...)\n\n`tiny_angular_dense` is a function that generate a lux model.\n\n\n\n\n\n","category":"method"}]
}
